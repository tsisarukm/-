{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko9XxVaeqOQF"
      },
      "source": [
        "# Лабораторная работа 2\n",
        "\n",
        "## Линейная регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajNyg1s8qOQJ"
      },
      "source": [
        "### Метод наименьших квадратов: постановка задачи\n",
        "\n",
        "Рассмотрим систему уравнений $Xa = y$, в которой $a$ — столбец неизвестных. Её можно переписать в векторном виде\n",
        "$$x_1 a_1 + x_2 a_2 + \\ldots + x_k a_k = y,$$\n",
        "где $x_1,\\ldots,x_k$ — столбцы матрицы $X$. Таким образом, решить исходную систему означает найти линейную комбинацию векторов $x_1,\\ldots,x_k$, равную правой части. Но что делать, если такой линейной комбинации не существует? Геометрически это означает, что вектор $y$ не лежит в подпространстве $U = \\langle x_1,\\ldots, x_k\\rangle$. В этом случае мы можем найти *псевдорешение*: вектор коэффициентов $\\hat{a}$, для которого линейная комбинация $x_1 \\hat{a}_1 + x_2 \\hat{a}_2 + \\ldots + x_k \\hat{a}_k$ хоть и не равна в точности $y$, но является наилучшим приближением — то есть ближайшей к $y$ точкой $\\hat{y}$ подпространства $U$ (иными словами, ортогональной проекцией $y$ на это подпростанство). Итак, цель наших исканий можно сформулировать двумя эквивалентными способами:\n",
        "\n",
        "1. Найти вектор $\\hat{a}$, для которого длина разности $|X\\hat{a} - y|$ минимальна (отсюда название \"метод наименьших квадратов\");\n",
        "2. Найти ортогональную проекцию $\\hat{y}$ вектора $y$ на подпространство $U$ и представить её в виде $X\\hat{a}$.\n",
        "\n",
        "Далее мы будем предполагать, что векторы $x_1,\\ldots,x_k$ линейно независимы (если нет, то сначала имеет смысл выделить максимальную линейно независимую подсистему).\n",
        "На лекциях было показано, что в этом случае проекция вектора $y$ на подпространство $U = \\langle x_1,\\ldots, x_k\\rangle$ записывается в виде\n",
        "$$\\hat{y} = X\\left(X^TX\\right)^{-1}X^Ty$$\n",
        "и, соответственно, искомый вектор $\\hat{a}$ равен\n",
        "$$\\hat{a} = \\left(X^TX\\right)^{-1}X^Ty.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofclUL48qOQK"
      },
      "source": [
        "### Задача линейной регрессии\n",
        "\n",
        "Начнём с примера. Допустим, вы хотите найти зависимость среднего балла S студента ФКН от его роста H, веса W, длины волос L и N — количества часов, которые он ежедневно посвящает учёбе. Представьте, что мы измерили все эти параметры для $n$ студентов и получили наборы значений: $S_1,\\ldots, S_n$, $H_1,\\ldots, H_n$ и так далее.\n",
        "\n",
        "Тут можно подбирать много разных умных моделей, но начать имеет смысл с самой простой, линейной:\n",
        "$$S = a_1H + a_2W + a_3L + a_4N + a_5.$$\n",
        "Конечно, строгой линейной зависимости нет (иначе можно было бы радостно упразднить экзамены), но мы можем попробовать подобрать коэффициенты $a_1, a_2, a_3, a_4, a_5$, для которых отклонение правой части от наблюдаемых было бы наименьшим:\n",
        "$$\\sum_{i=1}^n\\left(S_i - ( a_1H_i + a_2W_i + a_3L_i + a_4N_i + a_5)\\right)^2 \\longrightarrow \\min$$\n",
        "И сразу видно, что мы получили задачу на метод наименьших квадратов! А именно, у нас\n",
        "$$X =\n",
        "\\begin{pmatrix}\n",
        "H_1 & W_1 & L_1 & N_1 & 1\\\\\n",
        "H_2 & W_2 & L_2 & N_2 & 1\\\\\n",
        "\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n",
        "H_n & W_n & L_n & N_n & 1\n",
        "\\end{pmatrix},\\qquad y=\n",
        "\\begin{pmatrix}\n",
        "S_1\\\\ S_2\\\\ \\vdots \\\\ S_n\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "Решая эту задачу с помощью уже известных формул, получаем оценки коэффициентов $\\hat{a}_i$ ($i = 1\\ldots,5$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGHDJWxTqOQL"
      },
      "source": [
        "Теперь проговорим общую постановку задачи линейной регрессии. У нас есть $k$ переменных $x_1,\\ldots,x_k$ (\"регрессоров\"), через которые мы хотим выразить \"объясняемую переменную\" $y$:\n",
        "$$y = a_1x_1 + a_2x_2 + \\ldots + a_kx_k.$$\n",
        "Значения всех переменных мы измерили $n$ раз (у $n$ различных объектов,  в $n$ различных моментов времени — это зависит от задачи). Подставим эти данные в предыдущее равенство:\n",
        "$$\\begin{pmatrix}\n",
        "y_1\\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
        "\\end{pmatrix} =\n",
        "a_1\\begin{pmatrix}\n",
        "x_{11} \\\\ x_{21} \\\\ \\vdots \\\\ x_{n1} \\end{pmatrix} + a_2\\begin{pmatrix}\n",
        "x_{12} \\\\ x_{22} \\\\ \\vdots \\\\ x_{n2} \\end{pmatrix} + \\ldots + a_k\\begin{pmatrix}\n",
        "x_{1k} \\\\ x_{2k} \\\\ \\vdots \\\\ x_{nk} \\end{pmatrix}$$\n",
        "(здесь $x_{ij}$ — это значение $j$-го признака на $i$-м измерении). Это удобно переписать в матричном виде:\n",
        "$$\\begin{pmatrix}\n",
        "x_{11} & x_{12} & \\ldots & x_{1k}\\\\\n",
        "x_{21} & x_{22} & \\ldots & x_{2k}\\\\\n",
        "\\dots & \\dots & \\dots & \\dots\\\\\n",
        "x_{n1} & x_{n2} & \\ldots & x_{nk}\n",
        "\\end{pmatrix} \\cdot\n",
        "\\begin{pmatrix}\n",
        "a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_k\n",
        "\\end{pmatrix} =\n",
        "\\begin{pmatrix}\n",
        "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
        "\\end{pmatrix}$$\n",
        "или коротко $Xa = y$. Поскольку на практике эта система уравнений зачастую не имеет решения (ибо зависимости в жизни редко бывают действительно линейными), методом наименьших квадратов ищется псевдорешение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M_D0USiqOQL"
      },
      "source": [
        "### Оценка качества. Обучение и тест\n",
        "\n",
        "После того как вы построили регрессию и получили какую-то зависимость объясняемой переменной от регрессоров, настаёт время оценить качество регрессии. Есть много разных функционалов качества; мы пока будем говорить только о самом простом и очевидном из них: о среднеквадратичной ошибке (mean square error). Она равна\n",
        "$$\\frac1{n}|X\\hat{a} - y|^2 = \\frac1{n}\\sum_{i=1}^n\\left(\\hat{a}_1x_{i1} + \\hat{a}_2x_{i2} + \\ldots + \\hat{a}_kx_{ik} - y_i\\right)^2.$$\n",
        "\n",
        "В целом хочется искать модели с наименьшей mean square error на имеющихся данных. Однако слишком фанатичная гонка за минимизацией ошибки может привести к печальным последствиям, в чём вам предстоит убедиться в ходе выполнения этой лабораторной.\n",
        "\n",
        "Чтобы не попадать в эту ловушку, данные обычно делят на обучающие (по которым строят модель и оценивают коэффициенты) и тестовые. Лучшей стоит счесть ту модель, для которой значение функционала качества будет меньше."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_HMy6kNqOQM"
      },
      "source": [
        "### Правила оформления графиков\n",
        "При работе с данными часто неудобно делать какие-то выводы, если смотреть на таблицу и числа в частности, поэтому важно уметь визуализировать данные.\n",
        "\n",
        "У matplotlib, конечно же, есть [документация](https://matplotlib.org/users/index.html) с большим количеством [примеров](https://matplotlib.org/examples/), но для начала достаточно знать про несколько основных типов графиков:\n",
        "- plot — обычный поточечный график, которым можно изображать кривые или отдельные точки;\n",
        "- hist — гистограмма, показывающая распределение некоторой величины;\n",
        "- scatter — график, показывающий взаимосвязь двух величин;\n",
        "- bar — столбцовый график, показывающий взаимосвязь количественной величины от категориальной.\n",
        "\n",
        "Ещё одна библиотека для визуализации: [seaborn](https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html). Это надстройка над matplotlib, иногда удобнее и красивее делать визуализации через неё.\n",
        "\n",
        "При выполнении этой лабораторной вы столкнётесь с необходимостью рисовать большое количество графиков. Не забывайте про базовые принципы построения приличных графиков:\n",
        "- оси должны быть подписаны, причём не слишком мелко;\n",
        "- у графика должно быть название;\n",
        "- если изображено несколько графиков, то необходима поясняющая легенда;\n",
        "- для точек из разных выборок необходимо использовать разные цвета;\n",
        "- все линии на графиках должны быть чётко видны (нет похожих цветов или цветов, сливающихся с фоном);\n",
        "- если отображена величина, имеющая очевидный диапазон значений (например, проценты могут быть от 0 до 100), то желательно масштабировать ось на весь диапазон значений (исключением является случай, когда вам необходимо показать малое отличие, которое незаметно в таких масштабах).\n",
        "\n",
        "Помните, что проверяющий имеет право снизить оценку за неопрятные графики."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jf2gQapqOQM"
      },
      "source": [
        "### Формат сдачи\n",
        "Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием. Сам ноутбук называйте в формате homework-practice-02-linregr-Username.ipynb, где Username — ваша фамилия."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKY8bcLTqOQN"
      },
      "source": [
        "### Задание 1. Метод наименьших квадратов (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRlWe7KKqOQN"
      },
      "source": [
        "Скачайте файлы ``train.txt`` и ``test.txt``. В первом из них находится обучающая выборка, а во втором — тестовая. Каждый из файлов содержит два столбца чисел, разделённых пробелами: в первом — $n$ точек (значения аргумента $x$), во втором — значения некоторой функции $y = f(x)$ в этих точках, искажённые случайным шумом. Ваша задача — по обучающей выборке подобрать функцию $y = g(x)$, пристойно приближающую неизвестную вам зависимость."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv_ustjBqOQO"
      },
      "source": [
        "Загрузим обучающие и тестовые данные (не забудьте ввести правильный путь!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zsbRjiVQqOQO"
      },
      "outputs": [],
      "source": [
        "data_train = numpy.loadtxt('.../train.txt', delimiter=',')\n",
        "data_test = numpy.loadtxt('.../test.txt', delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nMMXMn9qOQP"
      },
      "source": [
        "**0. [0 баллов]** Разделим значения $x$ и $y$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j8mHS807qOQQ"
      },
      "outputs": [],
      "source": [
        "X_train = data_train[:,0]\n",
        "y_train = data_train[:,1]\n",
        "\n",
        "# Сделайте то же для тестовой выборки\n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pGVEni9qOQQ"
      },
      "source": [
        "**1. [0.1 балла]** Найдите с помощью метода наименьших квадратов линейную функцию ($y = kx + b$), наилучшим образом приближающую неизвестную зависимость. Полезные функции: ``numpy.ones(n)`` для создания массива из единиц длины $n$ и ``numpy.concatenate((А, В), axis=1)`` для слияния двух матриц по столбцам (пара ``А`` и ``В`` превращается в матрицу ``[A B]``). Напечатайте этот многочлен в виде $kx+b$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uPlCOqE7qOQQ"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF-ivuwpqOQR"
      },
      "source": [
        "**2. [0.15 балла]** Нарисуйте на плоскости точки $(x_i, y_i)$ из обеих выборок и полученную линейную функцию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cW6-86trqOQR"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHX9676bqOQR"
      },
      "source": [
        "**3. [0.15 балла]** Глядя на данные, подумайте, многочленом какой степени можно было бы лучше всего приблизить эту функцию с точки зрения минимизации среднеквадратичной ошибки на обучающей выборке. Найдите этот многочлен и напечатайте его в виде $a_0 + a_1 x + \\ldots a_k x^k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqJQhc-UqOQR"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2SM-4sGqOQR"
      },
      "source": [
        "**4. [0.1 балла]** Нарисуйте его график на одном чертеже вместе с точками $(x_i, y_i)$ из обеих выборок."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AX0GL31qOQR"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdHazeiGqOQS"
      },
      "source": [
        "**5. [0.25 балла]** Для $k = 1,2,3,\\ldots,10$ найдите многочлен $\\hat{f}_k$ степени $k$, наилучшим образом приближающий неизвестную зависимость. Напечатайте найденные многочлены в виде $a_0 + a_1 x + \\ldots a_k x^k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w_y0Xn4AqOQS"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwl54m8wqOQS"
      },
      "source": [
        "**6. [1 балл]** Для каждого из них найдите среднеквадратическую ошибку на обучающих данных и на тестовых данных: $\\frac1{n}\\sum_{i=1}^n\\left( \\hat{f}_k(x_i) - y_i \\right)^2$ (в первом случае сумма ведётся по парам $(x_i, y_i)$ из обучающих данных, а во втором — по парам из тестовых данных)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J3cBlupqOQS"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpScehMYqOQS"
      },
      "source": [
        "**7. [0.75 балла]** Для $k = 1,2,3,4,5,6$ нарисуйте графики полученных многочленов на одном чертеже вместе с точками $(x_i, y_i)$ из обеих выборок (возможно, график стоит сделать побольше; это делается командой `plt.figure(figsize=(width, height))`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hofYSv0oqOQS"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ParzqKySqOQT"
      },
      "source": [
        "**8. [0.5 балла]** Что происходит с ошибкой при росте степени многочлена? Казалось бы, чем больше степень, тем более сложным будет многочлен и тем лучше он будет приближать нашу функцию. Подтверждают ли это ваши наблюдения? Как вам кажется, чем объясняется поведение ошибки на тестовых данных при $k = 10$? Как, по вашему мнению, в машинном обучении называется наблюдаемый вами эффект?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkyipwBZqOQT"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqDO2JvrqOQT"
      },
      "source": [
        "### Задание 2. Линейная регрессия (3.5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2SeG4JXqOQT"
      },
      "source": [
        "Скачайте файлы ``flats_moscow_mod.txt`` и ``flats_moscow_description.txt``. В первом из них содержатся данные о квартирах в Москве. Каждая строка содержит шесть характеристик некоторой квартиры, разделённые знаками табуляции; в первой строке записаны кодовые названия характеристик. Во втором файле приведены краткие описания признаков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujhqvIceqOQT"
      },
      "source": [
        "**0. [0 баллов]** Разделите выборку на обучающую и тестовую. Делать это лучше случайным образом (ведь вы не знаете, как создатели датасета упорядочили объекты); рекомендуем вам для этого функцию `sklearn.model_selection.train_test_split` с параметром `test_size=0.3`. Не забудьте зафиксировать параметр `random_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyTvlY47qOQT"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaNTUg0AqOQT"
      },
      "source": [
        "**1. [0.5 балла]** Вашей задачей будет построить с помощью метода наименьших квадратов (линейную) зависимость между ценой квартиры и остальными доступными параметрами. С помощью известных вам формул найдите регрессионные коэффициенты. Выпишите построенную модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XEljbR-wqOQT"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bJZj-FnqOQU"
      },
      "source": [
        "**2. [0.25 балла]** Какой смысл имеют их знаки? Согласуются ли они с вашими представлениями о жизни?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vktQsPdWqOQU"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmsbkMMzqOQU"
      },
      "source": [
        "**3. [0.25 балла]** Оцените качество приближения, вычислив среднеквадратическую ошибку на тестовой выборке."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W2zfjDBqOQU"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Hlx8rdo7qOQU"
      },
      "source": [
        "### Усложнение модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XYUBuuKqOQU"
      },
      "source": [
        "Конечно, никто не гарантирует, что объясняемая переменная (цена квартиры) зависит от остальных характеристик именно линейно. Зависимость может быть, например, квадратичной или логарифмической; больше того, могут быть важны не только отдельные признаки, но и их комбинации. Это можно учитывать, добавляя в качестве дополнительных признаков разные функции от уже имеющихся характеристик: их квадраты, логарифмы, попарные произведения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al0tSMRzqOQU"
      },
      "source": [
        "В этом задании вам нужно постараться улучшить качество модели, добавляя дополнительные признаки, являющиеся функциями от уже имеющихся. Но будьте осторожны: чрезмерное усложнение модели будет приводить к переобучению."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcMvA2eCqOQV"
      },
      "source": [
        "**4. [0.25 балла]** Опишите признаки, которые вы решили добавить. Объясните, чем был мотивирован выбор именно этих признаков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg80itXGqOQV"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzWK2iAAqOQV"
      },
      "source": [
        "**5. [2 балла]** Постройте с помощью метода наименьших квадратов (линейную) зависимость между ценой квартиры и новыми признаками. С помощью известных вам формул найдите регрессионные коэффициенты. Выпишите построенную модель."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBTFofg6qOQV"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZmgNGJ6qOQV"
      },
      "source": [
        "**6. [0.25 балла]** Оцените качество приближения, вычислив среднеквадратическую ошибку на тестовой выборке."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V6MvEGTqOQV"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLfo4lHxqOQV"
      },
      "source": [
        "### Задание 3. Регуляризация (3.5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYxo3T5qqOQV"
      },
      "source": [
        "Вспомним, что задача линейной регрессии формулируется как задача нахождения проекции вектора значений объясняемой переменной $y$ на линейную оболочку $\\langle x_1,\\ldots,x_k\\rangle$ векторов значений регрессоров. Если векторы $x_1,\\ldots,x_k$ линейно зависимы, то матрица $X^TX$ вырожденна и задача не будет решаться с помощью приведённой выше формулы. В жизни, по счастью, различные признаки редко бывают *в точности* линейно зависимы, однако во многих ситуациях они скоррелированы и становятся \"почти\" линейно зависимыми. Таковы, к примеру, зарплата человека, его уровень образования, цена машины и суммарная площадь недвижимости, которой он владеет. В этом случае матрица $X^TX$ будет близка к вырожденной, и это приводит к численной неустойчивости и плохому качеству решений; как следствие, будет иметь место переобучение. Один из симптомов этой проблемы — необычно большие по модулю компоненты вектора $a$.\n",
        "\n",
        "Есть много способов борьбы с этим злом. Один из них — регуляризация. Сейчас мы рассмотрим одну из её разновидностей — **L2-регуляризацию**. Идея в том, чтобы подправить матрицу $X^TX$, сделав её \"получше\". Например, это можно сделать, заменив её на $(X^TX + \\lambda E)$, где $\\lambda > 0$ — некоторый скаляр. Пожертвовав точностью на обучающей выборке, мы тем не менее получаем численно более стабильное псевдорешение $a = (X^TX + \\lambda E)^{-1}X^Ty$ и снижаем эффект переобучения. *Гиперпараметр* $\\lambda$ нужно подбирать, и каких-то универсальных способов это делать нет, но зачастую можно его подобрать таким, чтобы ошибка на тестовой выборке падала.\n",
        "\n",
        "Однако есть некоторые правила, которых стоит придерживаться при подборе коэффициента регуляризации. Обычно в такой ситуации всю выборку делят на три части: обучающую, *валидационную* и тестовую. Сначала по валидационной подбирают значение гиперпараметра, потом по обучающей строят модель, а по тестовой оценивают её итоговое качество. Кроме того, подбирать $\\lambda$ нужно по логарифметической сетке, чтобы узнать оптимальный порядок величины."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drc2I8QkqOQW"
      },
      "source": [
        "**1. [0.25 балла]** Почему не стоит подбирать коэффициент регуляризации по обучающей выборке? По тестовой выборке?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj18QOSMqOQW"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsVCD8uqqOQW"
      },
      "source": [
        "**2. [0 баллов]** Теперь давайте вспомним первую задачу. Если вы её сделали, то помните, что ошибка аппроксимации многочленом шестой степени довольно высокая. Попытаемся использовать регуляризацию при построении модели. Поделите обучающую выборку из первой задачи на две части, одну из которых объявите обучающей, а другую — валидационной (будьте осторожны при выборе `train_size`, в этой выборке не так много данных)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQw3VuFRqOQW"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xymHWtPDqOQW"
      },
      "source": [
        "**3. [0.75 балла]** Убедитесь, что, используя регуляризацию с хорошо подобранным коэффициентом $\\lambda$, ошибку на тестовой выборке можно сделать не больше, чем для многочлена оптимальной степени в модели без регуляризации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bl8KmMIUqOQW"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oURPN62hqOQW"
      },
      "source": [
        "**4. [0.25 балла]** Для этого $\\lambda$ сравните $\\det(X^TX)$ и $\\det(X^TX + \\lambda E)$. Теоретически объясните полученный результат, доказав, что $\\det(X^TX+\\lambda E) > 0$ при $\\lambda>0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N70vmy1KqOQW"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl0oq4uvqOQX"
      },
      "source": [
        "**5. [0.25 балла]** Нарисуйте на одном чертеже графики многочленов шестой степени, приближающих неизвестную функцию, для модели с регуляризацией и без."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fBvCVQYnqOQX"
      },
      "outputs": [],
      "source": [
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue7wuRSmqOQX"
      },
      "source": [
        "**6. [0.25 балла]** Чем первый из них выгодно отличается от второго?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkpFHQRjqOQX"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iMIX5tZqOQX"
      },
      "source": [
        "**7. [0.75 балла]** Пусть заданы $X\\in \\text{Mat}_{n \\times k}(\\mathbb{R})$ ($k \\leqslant n$), $y \\in \\mathbb{R}^n$, $\\lambda \\geqslant 0$, а также известно, что $\\text{rk}~X = k$. Решите теоретически следующую задачу оптимизации:\n",
        "$$|Xa - y|^2 + \\lambda|a|^2\\rightarrow\\min\\limits_{a \\in \\mathbb{R}^k}.$$\n",
        "(То есть найдите все векторы $a \\in \\mathbb{R}^k$, для которых выражение слева принимает наименьшее возможное значение.) В частности, докажите существование решений и укажите их количество в зависимости от входных данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn3zeYm8qOQX"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyty-nT4qOQX"
      },
      "source": [
        "Интуитивно это можно понимать так: мы ищем компромисс между минимизацией длины разности $|Xa - y|$ (то есть точностью решения задачи регрессии) и тем, чтобы компоненты вектора $a$ не становились слишком большими по модулю."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYVu62REqOQY"
      },
      "source": [
        "**8. [0.25 балла]** Попробуйте объяснить, почему регуляризовать (штрафовать за слишком большую абсолютную величину) коэффициент при константном признаке (который состоит из одних единиц) — плохая идея."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9huVuXRwqOQY"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_fJfTQmqOQY"
      },
      "source": [
        "**9. [0.5 балла]** Пусть теперь $\\text{rk}~X < k$. Всегда ли в этом случае существует решение? Если существует, то является ли оно единственным? Ответ обоснуйте."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXHRvj3kqOQY"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StfBsh05qOQY"
      },
      "source": [
        "**10. [0.25 балла]** Покажите теоретически, что если решений бесконечно много, то среди них обязательно найдутся решения со сколь угодно большими по модулю компонентами вектора $a$. Почему большие веса в линейной модели — плохо?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U46zoG88qOQY"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5AyMk6rqOQZ"
      },
      "source": [
        "### Задание 4. Онлайн-обучение линейной регрессии (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-JbvXzqOQZ"
      },
      "source": [
        "Раньше мы работали в ситуации, когда объекты $x_i$ и значения $y_i$ даны с самого начала и всегда доступны. Допустим теперь, что пары $(x_i, y_i)$ поступают к нам по одной и мы не можем себе позволить хранить их все в памяти (это может быть актуально, например, если вы пытаетесь обучить модель на устройстве со сравнительно небольшим количеством оперативной памяти: скажем, на мобильном телефоне или на бортовом компьютере спутника связи). В этом случае нам нужно уметь решать следующую задачу:\n",
        "\n",
        "**Известно:** решение задачи регрессии для датасета $(x_1, y_1),\\ldots,(x_t,y_t)$;\n",
        "\n",
        "**На вход поступает:** новая пара $(x_{t+1}, y_{t+1})$;\n",
        "\n",
        "**Требуется:** быстро (за время, не зависящее от $t$) отыскать решение задачи регрессии для расширенного датасета $(x_1, y_1),\\ldots,(x_t,y_t),(x_{t+1}, y_{t+1})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trXdoqPfqOQZ"
      },
      "source": [
        "Обозначим $X_{(t)} = (x_1\\ldots x_t)^T$ и $y_{(t)} = (y_1,\\ldots,y_t)^T$. Тогда, как мы хорошо помним, решение задачи регрессии для датасета $(x_1, y_1),\\ldots,(x_t,y_t)$ имеет вид $\\hat{a}_{(t)} = \\left(X^T_{(t)}X_{(t)}\\right)^{-1}X^T_{(t)}y_{(t)}$. Размеры матриц $X^T_{(t)}X_{(t)}$ и $X^T_{(t)}y_{(t)}$ не зависят от $t$, поэтому их мы, пожалуй, можем себе позволить хранить в памяти."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVMx4fCyqOQZ"
      },
      "source": [
        "**1. [1 балл]** И вот ваше первое задание в этом разделе: придумайте алгоритм, принимающий на вход матрицы $X^T_{(t)}X_{(t)}$ и $X^T_{(t)}y_{(t)}$, а также пару $(x_{t+1}, y_{t+1})$, и вычисляющий матрицы $X^T_{(t+1)}X_{(t+1)}$ и $X^T_{(t+1)}y_{(t+1)}$. Сложность вашего алгоритма не должна зависеть от $t$. Опишите ваш алгоритм теоретически и обоснуйте его сложность."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO3cSnAQqOQZ"
      },
      "source": [
        "╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roPL0vwzqOQZ"
      },
      "source": [
        "**2. [2 балла]** Теперь настало время написать немного кода и порисовать красивые картинки. Вам нужно будет реализовать симуляцию онлайн-обучения регрессии для задачи приближения функции (в данном случае $f_{true}(x) = 2x\\sin(5x) + x^2 - 1$; все значения искажены небольшим нормальным шумом) многочленом степени не выше 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehAVEhm6qOQa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "f_true = lambda x: 2*x*np.sin(5*x) + x**2 - 1 # this is the true function\n",
        "\n",
        "# We need this to make the plot of f_true:\n",
        "x_grid = np.linspace(-2,5,100) # 100 linearly spaced numbers\n",
        "x_grid_enl = np.hstack((x_grid.reshape((100,1))**j for j in range(6)))\n",
        "y_grid = f_true(x_grid)\n",
        "\n",
        "\n",
        "for i in range(200):\n",
        "\n",
        "    x_new = np.random.uniform(-2, 5)\n",
        "    y_new = f_true(x_new) + 2*np.random.randn()\n",
        "\n",
        "    #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "\n",
        "    # the rest of the code is just bells and whistles\n",
        "    if (i+1)%5==0:\n",
        "        clear_output(True)\n",
        "        plt.plot(x_grid,y_grid, color='blue', label='true f')\n",
        "        plt.scatter(x_new, y_new, color='red')\n",
        "\n",
        "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "\n",
        "        y_pred = #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "\n",
        "        plt.scatter(x_grid, y_pred, color='orange', linewidth=5, label='predicted f')\n",
        "\n",
        "        plt.legend(loc='upper left')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFW5dq7TqOQa"
      },
      "source": [
        "### Добавление. QR-разложение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YTcuQZqqOQa"
      },
      "source": [
        "**QR-разложением** матрицы $A$ (не обязательно квадратной) мы будем называть её представление в виде $A = QR$, где $Q$ — матрица с ортонормированными столбцами, а $R$ — верхнетреугольная матрица.\n",
        "\n",
        "Смысл QR-разложения следующий. Пусть $a_1,\\ldots,a_m$ — столбцы матрицы $A$, $q_1,\\ldots,q_t$ — столбцы матрицы $Q$. Тогда $q_1,\\ldots,q_t$ — это ортонормированный базис в подпространстве, являющемся линейной оболочкой векторов $a_1,\\ldots,a_m$, а в матрице $R$ записаны коэффициенты, с помощью которых $a_i$ выражаются через $q_1,\\ldots,q_t$.\n",
        "\n",
        "Находить QR-разложение заданной матрицы можно разными способами. Мы познакомим вас не с самым лучшим из них, но по крайней мере с наиболее простым концептуально. Заметим, что ортогональный базис линейной оболочки можно найти с помощью ортогонализации Грама-Шмидта. При этом коэффициенты из матрицы $R$ получаются в качестве побочного продукта этого процесса:\n",
        "\n",
        "```python\n",
        "for j = 1...n:\n",
        "    q_j = a_j\n",
        "    for i = 1,...,j-1:\n",
        "        r_ij = (q_i, a_j)\n",
        "        q_j = q_j - r_ij * q_i\n",
        "    r_jj = |q_j|\n",
        "    if r_jj == 0: # a_j in <a_1,...,a_j-1>\n",
        "        # What would you do in this case?..\n",
        "    q_j = q_j / r_jj\n",
        "```\n",
        "\n",
        "Для нахождения QR-разложения вы можете использовать библиотечную функцию `scipy.linalg.qr`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpwq9fXVqOQa"
      },
      "source": [
        "Поскольку лабораторная про линейную регрессию, не так-то просто замять вопрос о том, какое же отношение QR-разложение имеет к задаче регрессии. Упомянем одно из возможных применений.\n",
        "\n",
        "Допустим, мы нашли QR-разложение матрицы $X$, а именно: $X = QR$. Тогда\n",
        "$$X^TX = (QR)^T(QR) = R^TQ^TQR = R^TR$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAw3yCcDqOQa"
      },
      "source": [
        "Поскольку в задаче регрессии матрица $X$ обычного полного ранга (то есть её столбцы линейно независимы), матрица $R$ будет квадратной. Благодаря этому нашу обычную формулу для набора регрессионных коэффициентов $\\hat{a}$ можно переписать в следующем виде:\n",
        "\n",
        "$$\\hat{a} = (X^TX)^{-1}X^Ty = (R^TR)^{-1}(QR)^Ty = R^{-1}(R^T)^{-1}R^TQ^Ty = R^{-1}Q^Ty$$\n",
        "\n",
        "Как видите, формула стала проще. Более того, зачастую обращение матрицы $R$ может быть численно более устойчиво, чем обращение матрицы $X^TX$."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}